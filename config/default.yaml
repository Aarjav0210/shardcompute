# ShardCompute Configuration
# Distributed Tensor Parallelism for Apple Silicon

coordinator:
  host: "0.0.0.0"
  port: 8080
  heartbeat_interval_seconds: 5
  heartbeat_timeout_seconds: 15

worker:
  collective_port_base: 9000  # Worker N uses port 9000 + N for peer connections
  collective_timeout_seconds: 30
  buffer_size: 1048576  # 1MB buffer for network operations

parallelism:
  tensor_parallel_size: 2  # Number of workers for tensor parallelism
  topology: "ring"  # Options: ring, tree, direct (for 2 workers)

model:
  name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  cache_dir: "./model_cache"
  shards_dir: "./model_shards"
  
  # Model architecture (for weight sharding)
  hidden_size: 2048
  intermediate_size: 5632
  num_heads: 32
  num_layers: 22
  vocab_size: 32000
  max_position_embeddings: 2048
  rms_norm_eps: 1e-5

serialization:
  compression_enabled: true
  compression_threshold_bytes: 1048576  # 1MB - compress tensors larger than this

logging:
  level: "INFO"
  format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
  file: null  # Set to path for file logging

inference:
  max_seq_length: 512
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
