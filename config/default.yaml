# ShardCompute Configuration
# Distributed Tensor Parallelism for Apple Silicon

coordinator:
  host: "0.0.0.0"
  port: 8000                      # Changed from 8080 to match COMMUNICATION_OUTLINE
  public_host: "127.0.0.1"        # Public address for DHT peer strings
  dht_port: 31337                 # DHT bootstrap port
  heartbeat_interval_seconds: 30  # Changed from 5 to match COMMUNICATION_OUTLINE
  heartbeat_timeout_seconds: 60   # Changed from 15 to match COMMUNICATION_OUTLINE
  stale_cleanup_interval: 30      # Background cleanup loop interval
  tokenizer_path: "./model_cache_mlx_Llama_3b_Instruct_4bit"  # Enable /inference/text

worker:
  transport: "ws_relay"  # Options: "tcp" (direct peer-to-peer), "ws_relay" (relay via coordinator)
  collective_port_base: 9000  # Worker N uses port 9000 + N for peer connections
  collective_timeout_seconds: 30
  buffer_size: 1048576  # 1MB buffer for network operations

parallelism:
  mode: "pipeline"
  # mode: "tensor"  # Options: "tensor" (TP - all workers have all layers with partial weights),
                   #          "pipeline" (PP - each worker has full weights for a subset of layers)
  tensor_parallel_size: 2  # Number of workers for tensor parallelism
  pipeline_parallel_size: 2  # Number of workers for pipeline parallelism
  topology: "ring"  # Options: ring, tree, direct (for 2 workers)

model:
  # ===== Active Model Configuration =====
  # Uncomment ONE of the model configs below to use it
  
  # Phi-2 (2.7B parameters, excellent quality, no auth required)
  name: "microsoft/phi-2"
  cache_dir: "./model_cache_phi2"
  shards_dir: "./model_shards_phi2"
  hidden_size: 2560
  intermediate_size: 10240
  num_heads: 32
  num_layers: 32
  vocab_size: 51200
  max_position_embeddings: 2048
  rms_norm_eps: 1e-5
  
  # # TinyLlama (1.1B parameters, small/fast, lower quality)
  # name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  # cache_dir: "./model_cache"
  # shards_dir: "./model_shards"
  # hidden_size: 2048
  # intermediate_size: 5632
  # num_heads: 32
  # num_layers: 22
  # vocab_size: 32000
  # max_position_embeddings: 2048
  # rms_norm_eps: 1e-5
  
  # # Llama 3.2 3B Instruct (3B parameters, high quality, requires HF auth)
  # name: "meta-llama/Llama-3.2-3B-Instruct"
  # cache_dir: "./model_cache_llama3_3b"
  # shards_dir: "./model_shards_llama3_3b"
  # hidden_size: 3072
  # intermediate_size: 8192
  # num_heads: 24
  # num_layers: 28
  # vocab_size: 128256
  # max_position_embeddings: 131072
  # rms_norm_eps: 1e-5
  
  # # Llama 3.2 8B Instruct (8B parameters, excellent quality, requires HF auth)
  # name: "meta-llama/Llama-3.2-8B-Instruct"
  # cache_dir: "./model_cache_llama3_8b"
  # shards_dir: "./model_shards_llama3_8b"
  # hidden_size: 4096
  # intermediate_size: 14336
  # num_heads: 32
  # num_layers: 32
  # vocab_size: 128256
  # max_position_embeddings: 131072
  # rms_norm_eps: 1e-5
  
  # # Mistral 7B Instruct (7B parameters, excellent quality, no auth required)
  # name: "mistralai/Mistral-7B-Instruct-v0.2"
  # cache_dir: "./model_cache_mistral_7b"
  # shards_dir: "./model_shards_mistral_7b"
  # hidden_size: 4096
  # intermediate_size: 14336
  # num_heads: 32
  # num_layers: 32
  # vocab_size: 32000
  # max_position_embeddings: 32768
  # rms_norm_eps: 1e-5
  
  # # Phi-3 Mini (3.8B parameters, high quality, no auth required)
  # name: "microsoft/Phi-3-mini-4k-instruct"
  # cache_dir: "./model_cache_phi3"
  # shards_dir: "./model_shards_phi3"
  # hidden_size: 3072
  # intermediate_size: 8192
  # num_heads: 32
  # num_layers: 32
  # vocab_size: 32064
  # max_position_embeddings: 4096
  # rms_norm_eps: 1e-5
  
  # # Phi-4 (14B parameters, state-of-the-art quality, no auth required)
  # name: "microsoft/phi-4"
  # cache_dir: "./model_cache_phi4"
  # shards_dir: "./model_shards_phi4"
  # hidden_size: 5120
  # intermediate_size: 17920
  # num_heads: 40
  # num_layers: 40
  # vocab_size: 100352
  # max_position_embeddings: 16384
  # rms_norm_eps: 1e-5

serialization:
  compression_enabled: false  # Disabled: float tensors have high entropy, zlib wastes CPU with minimal savings
  compression_threshold_bytes: 1048576  # 1MB - compress tensors larger than this

logging:
  level: "INFO"
  format: "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
  file: null  # Set to path for file logging

inference:
  max_seq_length: 512
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  eos_token_id: 50256  # Phi-2/Phi-3/Phi-4: 50256-100257 | TinyLlama/Llama/Mistral: 2
